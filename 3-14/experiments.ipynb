{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.font_manager\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from numpy import array\n",
    "from sklearn.feature_selection import chi2, RFECV\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "# import lightgbm as lgb\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, ElasticNet, Lars, Lasso, OrthogonalMatchingPursuit\n",
    "from sklearn.linear_model import ARDRegression, BayesianRidge\n",
    "from sklearn.linear_model import HuberRegressor, RANSACRegressor, TheilSenRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR, NuSVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, \n",
    "    HistGradientBoostingRegressor, \n",
    "    IsolationForest, \n",
    "    ExtraTreesRegressor, \n",
    "    AdaBoostRegressor\n",
    "    )\n",
    "from sklearn.linear_model import PoissonRegressor, TweedieRegressor, GammaRegressor\n",
    "import catboost\n",
    "import xgboost\n",
    "import lightgbm\n",
    "\n",
    "# import umap\n",
    "# from cuml.manifold import UMAP\n",
    "\n",
    "# if platform.processor() == 'arm':\n",
    "#     from sklearn.svm import SVR, NuSVR\n",
    "#     from sklearn.neighbors import KNeighborsClassifier\n",
    "# else:\n",
    "#     from cuml.svm import SVR\n",
    "#     from cuml.neighbors import KNeighborsClassifier\n",
    "#     import catboost as ctb\n",
    "\n",
    "import sklearn\n",
    "import mlflow\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "train_data_fname = 'train.csv'\n",
    "test_data_fname = 'test.csv'\n",
    "external_data_fname = 'external_data.csv'\n",
    "sample_submission_fname = 'sample_submission.csv'\n",
    "\n",
    "\n",
    "train_data_path = os.path.join(data_folder, train_data_fname)\n",
    "test_data_path = os.path.join(data_folder, test_data_fname)\n",
    "external_data_path = os.path.join(data_folder, external_data_fname)\n",
    "sample_data_path = os.path.join(data_folder, sample_submission_fname)\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "external_data = pd.read_csv(external_data_path)\n",
    "sample_data = pd.read_csv(sample_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'yield'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15289, 18), (10194, 17), (10194, 2), (777, 18))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape, sample_data.shape, external_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15289, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.loc[~train_data.duplicated()]\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = train_data.drop(['id','yield'], axis=1), train_data[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_(\n",
    "    X,\n",
    "    Y,\n",
    "    clf,\n",
    "    scaler=None,\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    run_info=None,\n",
    "    experiment_id=None,\n",
    "    verbose=1,\n",
    "):\n",
    "    model_name = clf.__str__().split(\"(\")[0]\n",
    "    if \"CatBoost\" in model_name:\n",
    "        model_name = \"CatBoost\"\n",
    "\n",
    "    run_timestamp = datetime.now().strftime(\"%H%M%S%d%m%Y\")\n",
    "\n",
    "    if run_info == None:\n",
    "        run_name_ = f\"{model_name}_{run_timestamp}\"\n",
    "    else:\n",
    "        run_name_ = f\"{model_name}_{run_info}_{run_timestamp}\"\n",
    "\n",
    "    if experiment_id:\n",
    "        try:\n",
    "            mlflow.start_run(run_name=run_name_, experiment_id=experiment_id)\n",
    "        except:\n",
    "            mlflow.end_run()\n",
    "            mlflow.start_run(run_name=run_name_, experiment_id=experiment_id)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            mlflow.start_run(run_name=run_name_)\n",
    "        except:\n",
    "            mlflow.end_run()\n",
    "            mlflow.start_run(run_name=run_name_)\n",
    "\n",
    "    cv = KFold(n_splits=n_splits, shuffle=shuffle, random_state=RANDOM_SEED)\n",
    "\n",
    "    if scaler:\n",
    "        pipeline = Pipeline(steps=[(\"scaler\", scaler), (\"clf\", clf)])\n",
    "    else:\n",
    "        pipeline = Pipeline(steps=[(\"clf\", clf)])\n",
    "\n",
    "    metrics = cross_validate(\n",
    "        estimator=pipeline,\n",
    "        X=X,\n",
    "        y=Y,\n",
    "        cv=cv,\n",
    "        scoring=[\n",
    "            \"neg_mean_absolute_error\",\n",
    "            \"neg_mean_absolute_percentage_error\",\n",
    "            # 'neg_mean_squared_error'\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Logging\n",
    "    metrics_dict = {\n",
    "        \"mae\":\n",
    "        np.mean(metrics[\"test_neg_mean_absolute_error\"] * -1),\n",
    "        \"mape\":\n",
    "        np.mean(metrics[\"test_neg_mean_absolute_percentage_error\"] * -1),\n",
    "        \"fit_time\":\n",
    "        np.mean(metrics[\"fit_time\"]),\n",
    "        \"inf_time\":\n",
    "        np.mean(metrics[\"score_time\"]),\n",
    "        \"min_mae\":\n",
    "        np.min(metrics[\"test_neg_mean_absolute_error\"] * -1),\n",
    "        \"min_mape\":\n",
    "        np.min(metrics[\"test_neg_mean_absolute_percentage_error\"] * -1),\n",
    "        \"max_mae\":\n",
    "        np.max(metrics[\"test_neg_mean_absolute_error\"] * -1),\n",
    "        \"max_mape\":\n",
    "        np.max(metrics[\"test_neg_mean_absolute_percentage_error\"] * -1),\n",
    "        \"std_mae\":\n",
    "        np.std(metrics[\"test_neg_mean_absolute_error\"] * -1),\n",
    "        \"std_mape\":\n",
    "        np.std(metrics[\"test_neg_mean_absolute_percentage_error\"] * -1),\n",
    "        \"var_mae\":\n",
    "        np.var(metrics[\"test_neg_mean_absolute_error\"] * -1),\n",
    "        \"var_mape\":\n",
    "        np.var(metrics[\"test_neg_mean_absolute_percentage_error\"] * -1),\n",
    "    }\n",
    "    model_params = clf.get_params()\n",
    "\n",
    "    mlflow.log_metrics(metrics=metrics_dict)\n",
    "    mlflow.log_params(params=model_params)\n",
    "\n",
    "    mlflow.end_run()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n {run_name_}\")\n",
    "        print(\n",
    "            f\"\\t MAE: \\t {metrics_dict['mae']:.4f} \\t ± {metrics_dict['std_mae']:.4f}; \\t min: {metrics_dict['min_mae']:.4f} \\t max: {metrics_dict['max_mae']:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\t MAPE: \\t {metrics_dict['mape']:.2%} \\t\\t ± {metrics_dict['std_mape']:.2%}; \\t min: {metrics_dict['min_mape']:.2%} \\t max: {metrics_dict['max_mape']:.2%}\"\n",
    "        )\n",
    "        print(f\"\\t Time: \\t {metrics_dict['fit_time']:.2f}s\")\n",
    "\n",
    "        print(f\"{80*'_'}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.create_experiment('baseline_rerun')\n",
    "# 972367584723232732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(),\n",
    "    SGDRegressor(),\n",
    "    ElasticNet(),\n",
    "    Lars(), \n",
    "    Lasso(),\n",
    "    BayesianRidge(),\n",
    "    ARDRegression(), \n",
    "    HuberRegressor(),\n",
    "    RANSACRegressor(),\n",
    "    TheilSenRegressor(),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(),\n",
    "    NuSVR(),\n",
    "    GaussianProcessRegressor(),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    HistGradientBoostingRegressor(),\n",
    "    IsolationForest(),\n",
    "    ExtraTreesRegressor(),\n",
    "    AdaBoostRegressor(),\n",
    "\n",
    "    lightgbm.LGBMRegressor(), \n",
    "    catboost.CatBoostRegressor(verbose=False),\n",
    "    xgboost.XGBRegressor(),\n",
    "    PoissonRegressor(),\n",
    "    TweedieRegressor(),\n",
    "    GammaRegressor()\n",
    "    \n",
    "]\n",
    "scalers = [\n",
    "    None,\n",
    "    StandardScaler(),\n",
    "    MinMaxScaler(),\n",
    "    RobustScaler(),\n",
    "    PowerTransformer(),\n",
    "    QuantileTransformer()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 14/27 [03:13<05:38, 26.03s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "for model in tqdm(models):\n",
    "    for scaler in scalers:\n",
    "        if scaler:\n",
    "            cross_validate_(\n",
    "                X = X,\n",
    "                Y = Y,\n",
    "                clf = model, \n",
    "                scaler = scaler,\n",
    "                n_splits=10,\n",
    "                shuffle = True,\n",
    "                experiment_id = '972367584723232732',\n",
    "                run_info = scaler.__str__()[:-2],\n",
    "                verbose=0\n",
    "            )\n",
    "        else:\n",
    "            cross_validate_(\n",
    "                X = X,\n",
    "                Y = Y,\n",
    "                clf = model, \n",
    "                n_splits=10,\n",
    "                scaler = None,\n",
    "                shuffle= True,\n",
    "                experiment_id='972367584723232732',\n",
    "                run_info = 'noscaler',\n",
    "                verbose=0,\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HistGradientBoostingRegressor_StandardScaler_13534805052023\n",
      "\t MAE: \t 353.8872 \t ± 12.9959; \t min: 338.3613 \t max: 383.0388\n",
      "\t MAPE: \t 6.28% \t\t ± 0.25%; \t min: 5.87% \t max: 6.77%\n",
      "\t Time: \t 0.35\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = HistGradientBoostingRegressor()\n",
    "scaler = StandardScaler()\n",
    "cross_validate_(\n",
    "                X = X,\n",
    "                Y = Y,\n",
    "                clf = model, \n",
    "                scaler = scaler,\n",
    "                n_splits=10,\n",
    "                run_info = scaler.__str__()[:-2],\n",
    "                experiment_id='972367584723232732',\n",
    "                verbose=1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
